"""
Project Name    :   Diabetic Retinopathy Detection
Description     :   This project aims at detecting the presence of Diabetic Retinopathy. Diabetic Retinopathy is an eye disease
                    that damages the retina of the eyes due to diabetes. Retinal fundus Image is fed as input to the model,
                    and the model classifies it as either Non-Referable Diabetics Retinopathy(NRDR) or Referable Diabetics
                    Retinopathy(RDR)
File Description:   Input_Pipeline.py takes the Indian Diabetic Retinopathy Image Dataset (IDRID) and creates
                    a tensorflow dataset object.
Contributors    :   Pavithran Pandiyan (st164390), Abirami Ravi (st164591)
"""


# -*- coding: utf-8 -*-
"""InputPipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RRrqRDxCvD80SmwNix4LVnVzHXuPOR-M
"""
"""
IDRID consists of retinal fundus images(in the folder "OriginalImages") and ground truths containing 
image name and severity grade of DR (in the folder "GroundTruths" as csv files. The dataset is already split
into Test and Training Dataset.
"""
#from google.colab import drive

#drive.mount ('/content/gdrive' )
#root_path = '/content/gdrive/My Drive/Retinopathy'

#!pip3 install tensorflow-gpu==2.0.0-beta0

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

#importing the required packages
import tensorflow as tf
import pathlib
import tensorflow.keras as k
import numpy as np
#from keras.models import Model
#from keras.layers import Input, Dense
root_path = "../Retinopathy"
#Location of the retinal fundus images in the dataset and their path objects
train_image_dir = root_path + "/OriginalImages/Training"
test_image_dir = root_path + "/OriginalImages/Testing"
train_image_path = pathlib.Path(train_image_dir)
test_image_path = pathlib.Path(test_image_dir)

#File location of the ground truths for the images
train_groundtruth_filename= root_path + "/GroundTruths/IDRiD_Disease Grading_Training Labels.csv"
test_groundtruth_filename = root_path + "/GroundTruths/IDRiD_Disease Grading_Testing Labels.csv"

#count the number of images in the training and test dataset folders
print(train_image_path)
print(test_image_path)
train_image_count = len(list(train_image_path.glob("*.jpg")))
test_image_count = len(list(test_image_path.glob("*.jpg")))
print("train images =",train_image_count)
print("test images =",test_image_count)

"""
Function Name       : decode_img
Function Description: Takes the content of the image input of any size and produces resized image of size 256*256
Arguments           : A Tensor of type String(content of img)
Returns             : Resized image of size 256*256
"""
def decode_img(img):
    # convert the compressed string to a 3D uint8 tensor
    img = tf.image.decode_jpeg(img, channels=3)
    # Use `convert_image_dtype` to convert to floats in the [0,1] range.
    img = tf.image.convert_image_dtype(img, tf.float32)
    # resize the image to the desired size.
    return tf.image.resize(img, [256, 256])


def process_path(file_path,labelx,labely):
    #print(label)
    # load the raw data from the file as a string
    img = tf.io.read_file(file_path)
    img = decode_img(img)
    return img, labelx, labely


def process_imgpath(file_path):
    #print(label)
   # load the raw data from the file as a string
    img = tf.io.read_file(file_path)
    img = decode_img(img)
    return img


def read_label_file(filewithpath,directory):
    a=[]
    b=[]
    c=[]
    i=1
    with open(filewithpath) as labels_file:
        for line in labels_file:
            if i is 1:
                i=0
                continue
            x = line.split(',')
            x[0] = directory+"/" + x[0]+".jpg"
            a.append(x[0])
            b.append(float(x[1]))
            c.append(float(x[2])) 
    return a,np.array(b, dtype=np.float32),np.array(c, dtype=np.float32)

train_img, train_label_Grade, train_label_Risk = read_label_file(train_groundtruth_filename,train_image_dir)
test_img, test_label_Grade, test_label_Risk = read_label_file(test_groundtruth_filename,test_image_dir)

train_labels = tf.data.Dataset.from_tensor_slices((train_label_Grade, train_label_Risk))
test_labels = tf.data.Dataset.from_tensor_slices((test_label_Grade, test_label_Risk))

train_img = tf.data.Dataset.from_tensor_slices(train_img)
test_img = tf.data.Dataset.from_tensor_slices(test_img)

train_img = train_img.map(process_imgpath, num_parallel_calls=None)
test_img = test_img.map(process_imgpath, num_parallel_calls=None)

train_ds = tf.data.Dataset.zip((train_img,train_labels))
test_ds = tf.data.Dataset.zip((test_img,test_labels))

#train_ds = train_ds.map(process_path, num_parallel_calls=None)
#test_ds = test_ds.map(process_path, num_parallel_calls=None)
train_ds = train_ds.shuffle(413).batch(10)
test_ds = test_ds.shuffle(103).batch(10)
validation_ds = train_ds.take(20)
#labeled_ds = labeled_ds.shuffle
#for image, labelx,labely in test_ds.take(2):
    #display.display(image)
#    print("Image shape: ", image.numpy().shape)
#    print("Label: ", labelx, labely)
#labeled_ds = labeled_ds.batch(64)
#labeled_ds.from_generator()

